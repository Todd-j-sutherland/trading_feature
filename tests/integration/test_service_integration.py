#!/usr/bin/env python3
"""
Comprehensive Service Integration Testing Framework

This module provides comprehensive integration testing for the trading microservices architecture,
testing service-to-service communication, error propagation, performance under load, and 
end-to-end workflows.

Author: Trading System Integration Test Framework
Date: September 14, 2025
"""

import asyncio
import json
import time
import unittest
import logging
import subprocess
import threading
import concurrent.futures
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import psutil
import redis
import aioredis
import sqlite3
from pathlib import Path
import sys
import os

# Add project root to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

@dataclass
class ServiceTestResult:
    """Result of a service integration test"""
    service_name: str
    test_name: str
    success: bool
    execution_time: float
    error_message: Optional[str] = None
    metrics: Optional[Dict] = None
    
@dataclass 
class CommunicationTest:
    """Service-to-service communication test configuration"""
    source_service: str
    target_service: str
    method: str
    params: Dict
    expected_result_type: str
    timeout: float = 30.0

class ServiceIntegrationTestFramework:
    """Comprehensive framework for testing microservice integration"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_url = redis_url
        self.services = [
            "market-data",
            "sentiment", 
            "prediction",
            "scheduler",
            "paper-trading",
            "ml-model"
        ]
        
        self.service_ports = {
            "market-data": "/tmp/trading_market-data.sock",
            "sentiment": "/tmp/trading_sentiment.sock", 
            "prediction": "/tmp/trading_prediction.sock",
            "scheduler": "/tmp/trading_scheduler.sock",
            "paper-trading": "/tmp/trading_paper-trading.sock",
            "ml-model": "/tmp/trading_ml-model.sock"
        }
        
        self.test_results: List[ServiceTestResult] = []
        self.performance_metrics: Dict[str, Any] = {}
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
    async def run_all_integration_tests(self) -> Dict[str, Any]:
        """Run comprehensive integration test suite"""
        self.logger.info("ðŸš€ Starting Comprehensive Service Integration Tests")
        
        start_time = time.time()
        overall_results = {
            'test_timestamp': datetime.now().isoformat(),
            'test_duration': 0,
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'test_categories': {}
        }
        
        try:
            # 1. Service Health and Availability Tests
            health_results = await self.test_service_health()
            overall_results['test_categories']['health_check'] = health_results
            
            # 2. Service-to-Service Communication Tests  
            communication_results = await self.test_service_communication()
            overall_results['test_categories']['communication'] = communication_results
            
            # 3. Error Propagation Tests
            error_results = await self.test_error_propagation()
            overall_results['test_categories']['error_propagation'] = error_results
            
            # 4. Performance Under Load Tests
            performance_results = await self.test_performance_under_load()
            overall_results['test_categories']['performance'] = performance_results
            
            # 5. End-to-End Workflow Tests
            e2e_results = await self.test_end_to_end_workflows()
            overall_results['test_categories']['end_to_end'] = e2e_results
            
            # 6. Failure Scenario Tests
            failure_results = await self.test_failure_scenarios()
            overall_results['test_categories']['failure_scenarios'] = failure_results
            
            # 7. Data Consistency Tests
            consistency_results = await self.test_data_consistency()
            overall_results['test_categories']['data_consistency'] = consistency_results
            
        except Exception as e:
            self.logger.error(f"Integration test suite failed: {e}")
            overall_results['suite_error'] = str(e)
            
        # Calculate summary statistics
        total_tests = sum(
            len(category.get('test_results', [])) 
            for category in overall_results['test_categories'].values()
        )
        
        passed_tests = sum(
            sum(1 for test in category.get('test_results', []) if test.get('success', False))
            for category in overall_results['test_categories'].values()
        )
        
        overall_results.update({
            'test_duration': time.time() - start_time,
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': total_tests - passed_tests,
            'success_rate': passed_tests / total_tests if total_tests > 0 else 0
        })
        
        self.logger.info(f"âœ… Integration test suite completed: {passed_tests}/{total_tests} tests passed")
        return overall_results
        
    async def test_service_health(self) -> Dict[str, Any]:
        """Test health and availability of all services"""
        self.logger.info("ðŸ¥ Testing Service Health and Availability")
        
        health_results = {
            'category': 'health_check',
            'test_results': [],
            'summary': {}
        }
        
        health_tasks = []
        for service_name in self.services:
            task = self._test_single_service_health(service_name)
            health_tasks.append(task)
            
        results = await asyncio.gather(*health_tasks, return_exceptions=True)
        
        for i, result in enumerate(results):
            service_name = self.services[i]
            
            if isinstance(result, Exception):
                test_result = {
                    'service': service_name,
                    'test': 'health_check',
                    'success': False,
                    'error': str(result),
                    'execution_time': 0
                }
            else:
                test_result = result
                
            health_results['test_results'].append(test_result)
            
        # Calculate health summary
        healthy_services = sum(1 for r in health_results['test_results'] if r['success'])
        total_services = len(self.services)
        
        health_results['summary'] = {
            'healthy_services': healthy_services,
            'total_services': total_services,
            'health_rate': healthy_services / total_services
        }
        
        self.logger.info(f"Health check completed: {healthy_services}/{total_services} services healthy")
        return health_results
        
    async def _test_single_service_health(self, service_name: str) -> Dict[str, Any]:
        """Test health of a single service"""
        start_time = time.time()
        
        try:
            # Test socket connectivity
            socket_path = self.service_ports.get(service_name)
            if not socket_path:
                raise ValueError(f"No socket path configured for {service_name}")
                
            # Attempt connection and health check
            reader, writer = await asyncio.wait_for(
                asyncio.open_unix_connection(socket_path),
                timeout=10.0
            )
            
            # Send health check request
            health_request = {
                'method': 'health',
                'params': {},
                'timestamp': time.time()
            }
            
            writer.write(json.dumps(health_request).encode())
            await writer.drain()
            
            # Read response
            response_data = await asyncio.wait_for(reader.read(8192), timeout=10.0)
            response = json.loads(response_data.decode())
            
            writer.close()
            await writer.wait_closed()
            
            execution_time = time.time() - start_time
            
            if response.get('status') == 'success':
                health_data = response.get('result', {})
                return {
                    'service': service_name,
                    'test': 'health_check',
                    'success': True,
                    'execution_time': execution_time,
                    'health_data': health_data,
                    'status': health_data.get('status', 'unknown'),
                    'uptime': health_data.get('uptime', 0)
                }
            else:
                return {
                    'service': service_name,
                    'test': 'health_check',
                    'success': False,
                    'execution_time': execution_time,
                    'error': response.get('error', 'Unknown health check error')
                }
                
        except Exception as e:
            return {
                'service': service_name,
                'test': 'health_check',
                'success': False,
                'execution_time': time.time() - start_time,
                'error': str(e)
            }
            
    async def test_service_communication(self) -> Dict[str, Any]:
        """Test service-to-service communication patterns"""
        self.logger.info("ðŸ“¡ Testing Service-to-Service Communication")
        
        communication_tests = [
            # Market data to prediction service
            CommunicationTest(
                source_service="market-data",
                target_service="prediction",
                method="get_market_data",
                params={"symbol": "CBA.AX"},
                expected_result_type="dict"
            ),
            
            # Sentiment to prediction service
            CommunicationTest(
                source_service="sentiment", 
                target_service="prediction",
                method="analyze_sentiment",
                params={"symbol": "CBA.AX"},
                expected_result_type="dict"
            ),
            
            # Prediction service batch operations
            CommunicationTest(
                source_service="prediction",
                target_service="prediction",
                method="generate_predictions",
                params={"symbols": ["CBA.AX", "WBC.AX"], "force_refresh": True},
                expected_result_type="dict"
            ),
            
            # Paper trading integration
            CommunicationTest(
                source_service="prediction",
                target_service="paper-trading", 
                method="execute_trade",
                params={"symbol": "CBA.AX", "action": "BUY", "quantity": 100},
                expected_result_type="dict"
            ),
            
            # Scheduler coordination
            CommunicationTest(
                source_service="scheduler",
                target_service="prediction",
                method="generate_predictions",
                params={"symbols": ["CBA.AX"]},
                expected_result_type="dict"
            )
        ]
        
        communication_results = {
            'category': 'communication',
            'test_results': [],
            'summary': {}
        }
        
        for comm_test in communication_tests:
            result = await self._test_service_communication(comm_test)
            communication_results['test_results'].append(result)
            
        # Calculate communication summary
        successful_comms = sum(1 for r in communication_results['test_results'] if r['success'])
        total_comms = len(communication_tests)
        
        communication_results['summary'] = {
            'successful_communications': successful_comms,
            'total_communications': total_comms,
            'communication_success_rate': successful_comms / total_comms
        }
        
        self.logger.info(f"Communication tests completed: {successful_comms}/{total_comms} successful")
        return communication_results
        
    async def _test_service_communication(self, comm_test: CommunicationTest) -> Dict[str, Any]:
        """Test a single service-to-service communication"""
        start_time = time.time()
        
        try:
            # Get target service socket
            socket_path = self.service_ports.get(comm_test.target_service)
            if not socket_path:
                raise ValueError(f"No socket path for {comm_test.target_service}")
                
            # Connect and send request
            reader, writer = await asyncio.wait_for(
                asyncio.open_unix_connection(socket_path),
                timeout=comm_test.timeout
            )
            
            request = {
                'method': comm_test.method,
                'params': comm_test.params,
                'timestamp': time.time()
            }
            
            writer.write(json.dumps(request).encode())
            await writer.drain()
            
            # Read response
            response_data = await asyncio.wait_for(
                reader.read(32768), 
                timeout=comm_test.timeout
            )
            response = json.loads(response_data.decode())
            
            writer.close()
            await writer.wait_closed()
            
            execution_time = time.time() - start_time
            
            # Validate response
            success = (
                response.get('status') == 'success' and
                'result' in response and
                isinstance(response['result'], dict if comm_test.expected_result_type == 'dict' else list)
            )
            
            return {
                'test': f"{comm_test.source_service}_to_{comm_test.target_service}_{comm_test.method}",
                'success': success,
                'execution_time': execution_time,
                'response_size': len(response_data),
                'error': None if success else response.get('error', 'Invalid response format')
            }
            
        except Exception as e:
            return {
                'test': f"{comm_test.source_service}_to_{comm_test.target_service}_{comm_test.method}",
                'success': False,
                'execution_time': time.time() - start_time,
                'error': str(e)
            }
            
    async def test_error_propagation(self) -> Dict[str, Any]:
        """Test error handling and propagation across services"""
        self.logger.info("âš ï¸ Testing Error Propagation and Handling")
        
        error_tests = [
            # Invalid symbol test
            {
                'test_name': 'invalid_symbol_handling',
                'service': 'market-data',
                'method': 'get_market_data',
                'params': {'symbol': 'INVALID.SYMBOL.TEST'},
                'expected_error_type': 'validation_error'
            },
            
            # Rate limiting test  
            {
                'test_name': 'rate_limiting',
                'service': 'prediction',
                'method': 'generate_predictions',
                'params': {'symbols': ['CBA.AX'] * 100},  # Excessive requests
                'expected_error_type': 'rate_limit_error'
            },
            
            # Timeout handling test
            {
                'test_name': 'timeout_handling',
                'service': 'sentiment',
                'method': 'analyze_sentiment',
                'params': {'symbol': 'CBA.AX', 'timeout': 0.001},  # Very short timeout
                'expected_error_type': 'timeout_error'
            }
        ]
        
        error_results = {
            'category': 'error_propagation',
            'test_results': [],
            'summary': {}
        }
        
        for error_test in error_tests:
            result = await self._test_error_handling(error_test)
            error_results['test_results'].append(result)
            
        # Calculate error handling summary
        proper_errors = sum(1 for r in error_results['test_results'] if r['proper_error_handling'])
        total_error_tests = len(error_tests)
        
        error_results['summary'] = {
            'proper_error_handling': proper_errors,
            'total_error_tests': total_error_tests,
            'error_handling_rate': proper_errors / total_error_tests
        }
        
        self.logger.info(f"Error propagation tests completed: {proper_errors}/{total_error_tests} proper error handling")
        return error_results
        
    async def _test_error_handling(self, error_test: Dict[str, Any]) -> Dict[str, Any]:
        """Test error handling for a specific scenario"""
        start_time = time.time()
        
        try:
            socket_path = self.service_ports.get(error_test['service'])
            if not socket_path:
                raise ValueError(f"No socket path for {error_test['service']}")
                
            reader, writer = await asyncio.wait_for(
                asyncio.open_unix_connection(socket_path),
                timeout=30.0
            )
            
            request = {
                'method': error_test['method'],
                'params': error_test['params'],
                'timestamp': time.time()
            }
            
            writer.write(json.dumps(request).encode())
            await writer.drain()
            
            response_data = await asyncio.wait_for(reader.read(32768), timeout=30.0)
            response = json.loads(response_data.decode())
            
            writer.close()
            await writer.wait_closed()
            
            execution_time = time.time() - start_time
            
            # Check if error was properly handled
            proper_error_handling = (
                response.get('status') == 'error' and
                'error' in response and
                len(response.get('error', '')) > 0
            )
            
            return {
                'test': error_test['test_name'],
                'service': error_test['service'],
                'success': True,  # Test executed successfully
                'proper_error_handling': proper_error_handling,
                'execution_time': execution_time,
                'error_message': response.get('error'),
                'expected_error_type': error_test['expected_error_type']
            }
            
        except Exception as e:
            return {
                'test': error_test['test_name'],
                'service': error_test['service'],
                'success': False,
                'proper_error_handling': False,
                'execution_time': time.time() - start_time,
                'error': str(e)
            }
            
    async def test_performance_under_load(self) -> Dict[str, Any]:
        """Test service performance under concurrent load"""
        self.logger.info("ðŸš€ Testing Performance Under Load")
        
        performance_results = {
            'category': 'performance',
            'test_results': [],
            'summary': {}
        }
        
        # Test concurrent prediction requests
        concurrent_requests = 10
        symbols = ['CBA.AX', 'WBC.AX', 'ANZ.AX', 'NAB.AX']
        
        start_time = time.time()
        
        # Create concurrent tasks
        tasks = []
        for i in range(concurrent_requests):
            symbol = symbols[i % len(symbols)]
            task = self._make_prediction_request(symbol, f"load_test_{i}")
            tasks.append(task)
            
        # Execute concurrent requests
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_time = time.time() - start_time
        
        # Analyze results
        successful_requests = sum(1 for r in results if isinstance(r, dict) and r.get('success'))
        failed_requests = concurrent_requests - successful_requests
        
        if successful_requests > 0:
            avg_response_time = sum(
                r.get('execution_time', 0) for r in results 
                if isinstance(r, dict) and r.get('success')
            ) / successful_requests
        else:
            avg_response_time = 0
            
        throughput = successful_requests / total_time if total_time > 0 else 0
        
        performance_result = {
            'test': 'concurrent_prediction_load',
            'concurrent_requests': concurrent_requests,
            'successful_requests': successful_requests,
            'failed_requests': failed_requests,
            'total_time': total_time,
            'avg_response_time': avg_response_time,
            'throughput_per_second': throughput,
            'success_rate': successful_requests / concurrent_requests,
            'success': successful_requests >= concurrent_requests * 0.8  # 80% success threshold
        }
        
        performance_results['test_results'].append(performance_result)
        
        # System resource monitoring during load
        system_metrics = self._collect_system_metrics()
        performance_results['system_metrics'] = system_metrics
        
        performance_results['summary'] = {
            'load_test_passed': performance_result['success'],
            'peak_throughput': throughput,
            'avg_response_time': avg_response_time
        }
        
        self.logger.info(f"Performance test completed: {throughput:.2f} requests/sec, {avg_response_time:.3f}s avg response")
        return performance_results
        
    async def _make_prediction_request(self, symbol: str, request_id: str) -> Dict[str, Any]:
        """Make a single prediction request for load testing"""
        start_time = time.time()
        
        try:
            socket_path = self.service_ports.get('prediction')
            
            reader, writer = await asyncio.wait_for(
                asyncio.open_unix_connection(socket_path),
                timeout=30.0
            )
            
            request = {
                'method': 'generate_single_prediction',
                'params': {'symbol': symbol, 'force_refresh': True},
                'request_id': request_id,
                'timestamp': time.time()
            }
            
            writer.write(json.dumps(request).encode())
            await writer.drain()
            
            response_data = await asyncio.wait_for(reader.read(32768), timeout=30.0)
            response = json.loads(response_data.decode())
            
            writer.close()
            await writer.wait_closed()
            
            execution_time = time.time() - start_time
            
            return {
                'request_id': request_id,
                'symbol': symbol,
                'success': response.get('status') == 'success',
                'execution_time': execution_time,
                'response_size': len(response_data)
            }
            
        except Exception as e:
            return {
                'request_id': request_id,
                'symbol': symbol,
                'success': False,
                'execution_time': time.time() - start_time,
                'error': str(e)
            }
            
    def _collect_system_metrics(self) -> Dict[str, Any]:
        """Collect system resource metrics"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            return {
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'memory_available_gb': memory.available / (1024**3),
                'disk_percent': disk.percent,
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            return {'error': str(e)}
            
    async def test_end_to_end_workflows(self) -> Dict[str, Any]:
        """Test complete end-to-end trading workflows"""
        self.logger.info("ðŸ”„ Testing End-to-End Workflows")
        
        e2e_results = {
            'category': 'end_to_end',
            'test_results': [],
            'summary': {}
        }
        
        # Test complete trading workflow
        workflow_result = await self._test_complete_trading_workflow()
        e2e_results['test_results'].append(workflow_result)
        
        # Test market analysis workflow
        analysis_result = await self._test_market_analysis_workflow()
        e2e_results['test_results'].append(analysis_result)
        
        successful_workflows = sum(1 for r in e2e_results['test_results'] if r['success'])
        total_workflows = len(e2e_results['test_results'])
        
        e2e_results['summary'] = {
            'successful_workflows': successful_workflows,
            'total_workflows': total_workflows,
            'workflow_success_rate': successful_workflows / total_workflows
        }
        
        self.logger.info(f"End-to-end tests completed: {successful_workflows}/{total_workflows} workflows successful")
        return e2e_results
        
    async def _test_complete_trading_workflow(self) -> Dict[str, Any]:
        """Test complete trading workflow: market data â†’ analysis â†’ prediction â†’ trading"""
        start_time = time.time()
        workflow_steps = []
        
        try:
            symbol = "CBA.AX"
            
            # Step 1: Get market data
            market_data_result = await self._call_service('market-data', 'get_market_data', {'symbol': symbol})
            workflow_steps.append({
                'step': 'market_data_fetch',
                'success': market_data_result.get('success', False),
                'execution_time': market_data_result.get('execution_time', 0)
            })
            
            if not market_data_result.get('success'):
                raise Exception("Market data fetch failed")
                
            # Step 2: Analyze sentiment
            sentiment_result = await self._call_service('sentiment', 'analyze_sentiment', {'symbol': symbol})
            workflow_steps.append({
                'step': 'sentiment_analysis',
                'success': sentiment_result.get('success', False),
                'execution_time': sentiment_result.get('execution_time', 0)
            })
            
            # Step 3: Generate prediction
            prediction_result = await self._call_service('prediction', 'generate_single_prediction', {
                'symbol': symbol, 
                'force_refresh': True
            })
            workflow_steps.append({
                'step': 'prediction_generation',
                'success': prediction_result.get('success', False),
                'execution_time': prediction_result.get('execution_time', 0)
            })
            
            if not prediction_result.get('success'):
                raise Exception("Prediction generation failed")
                
            # Step 4: Execute paper trade (if prediction is BUY)
            prediction_data = prediction_result.get('result', {})
            if prediction_data.get('action') in ['BUY', 'STRONG_BUY']:
                trade_result = await self._call_service('paper-trading', 'execute_trade', {
                    'symbol': symbol,
                    'action': 'BUY',
                    'quantity': 100
                })
                workflow_steps.append({
                    'step': 'trade_execution',
                    'success': trade_result.get('success', False),
                    'execution_time': trade_result.get('execution_time', 0)
                })
            else:
                workflow_steps.append({
                    'step': 'trade_execution',
                    'success': True,
                    'execution_time': 0,
                    'note': 'No trade executed - prediction was not BUY'
                })
                
            total_time = time.time() - start_time
            all_steps_successful = all(step['success'] for step in workflow_steps)
            
            return {
                'test': 'complete_trading_workflow',
                'success': all_steps_successful,
                'execution_time': total_time,
                'workflow_steps': workflow_steps,
                'symbol': symbol
            }
            
        except Exception as e:
            return {
                'test': 'complete_trading_workflow',
                'success': False,
                'execution_time': time.time() - start_time,
                'error': str(e),
                'workflow_steps': workflow_steps
            }
            
    async def _test_market_analysis_workflow(self) -> Dict[str, Any]:
        """Test market analysis workflow: batch predictions â†’ analysis â†’ reporting"""
        start_time = time.time()
        
        try:
            symbols = ['CBA.AX', 'WBC.AX', 'ANZ.AX']
            
            # Generate batch predictions
            batch_result = await self._call_service('prediction', 'generate_predictions', {
                'symbols': symbols,
                'force_refresh': True
            })
            
            if not batch_result.get('success'):
                raise Exception("Batch prediction failed")
                
            # Get prediction summary/buy rate
            buy_rate_result = await self._call_service('prediction', 'get_buy_rate', {})
            
            total_time = time.time() - start_time
            
            # Validate results
            batch_data = batch_result.get('result', {})
            predictions = batch_data.get('predictions', {})
            
            successful_predictions = sum(
                1 for pred in predictions.values() 
                if isinstance(pred, dict) and 'error' not in pred
            )
            
            success = (
                batch_result.get('success', False) and
                buy_rate_result.get('success', False) and
                successful_predictions >= len(symbols) * 0.8  # 80% success rate
            )
            
            return {
                'test': 'market_analysis_workflow',
                'success': success,
                'execution_time': total_time,
                'symbols_analyzed': len(symbols),
                'successful_predictions': successful_predictions,
                'batch_data': batch_data.get('summary', {}),
                'buy_rate': buy_rate_result.get('result', {})
            }
            
        except Exception as e:
            return {
                'test': 'market_analysis_workflow',
                'success': False,
                'execution_time': time.time() - start_time,
                'error': str(e)
            }
            
    async def _call_service(self, service_name: str, method: str, params: Dict) -> Dict[str, Any]:
        """Helper method to call a service"""
        start_time = time.time()
        
        try:
            socket_path = self.service_ports.get(service_name)
            if not socket_path:
                raise ValueError(f"No socket path for {service_name}")
                
            reader, writer = await asyncio.wait_for(
                asyncio.open_unix_connection(socket_path),
                timeout=30.0
            )
            
            request = {
                'method': method,
                'params': params,
                'timestamp': time.time()
            }
            
            writer.write(json.dumps(request).encode())
            await writer.drain()
            
            response_data = await asyncio.wait_for(reader.read(32768), timeout=30.0)
            response = json.loads(response_data.decode())
            
            writer.close()
            await writer.wait_closed()
            
            return {
                'success': response.get('status') == 'success',
                'result': response.get('result'),
                'error': response.get('error'),
                'execution_time': time.time() - start_time
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'execution_time': time.time() - start_time
            }
            
    async def test_failure_scenarios(self) -> Dict[str, Any]:
        """Test service behavior under various failure scenarios"""
        self.logger.info("ðŸ’¥ Testing Failure Scenarios and Recovery")
        
        failure_results = {
            'category': 'failure_scenarios',
            'test_results': [],
            'summary': {}
        }
        
        # Test Redis connection failure simulation
        redis_failure_result = await self._test_redis_failure_handling()
        failure_results['test_results'].append(redis_failure_result)
        
        # Test service unavailability handling
        unavailable_result = await self._test_service_unavailability()
        failure_results['test_results'].append(unavailable_result)
        
        # Test resource exhaustion handling
        resource_result = await self._test_resource_exhaustion()
        failure_results['test_results'].append(resource_result)
        
        successful_recoveries = sum(1 for r in failure_results['test_results'] if r.get('recovery_successful', False))
        total_failure_tests = len(failure_results['test_results'])
        
        failure_results['summary'] = {
            'successful_recoveries': successful_recoveries,
            'total_failure_tests': total_failure_tests,
            'recovery_rate': successful_recoveries / total_failure_tests if total_failure_tests > 0 else 0
        }
        
        self.logger.info(f"Failure scenario tests completed: {successful_recoveries}/{total_failure_tests} recovered successfully")
        return failure_results
        
    async def _test_redis_failure_handling(self) -> Dict[str, Any]:
        """Test how services handle Redis connection failures"""
        start_time = time.time()
        
        try:
            # First, verify services work with Redis
            before_result = await self._call_service('prediction', 'get_buy_rate', {})
            
            # Note: In a real test environment, we would temporarily disable Redis
            # For now, we simulate by testing graceful degradation
            
            return {
                'test': 'redis_failure_handling',
                'success': True,  # Test executed
                'recovery_successful': before_result.get('success', False),
                'execution_time': time.time() - start_time,
                'note': 'Redis failure simulation requires test environment setup'
            }
            
        except Exception as e:
            return {
                'test': 'redis_failure_handling',
                'success': False,
                'recovery_successful': False,
                'execution_time': time.time() - start_time,
                'error': str(e)
            }
            
    async def _test_service_unavailability(self) -> Dict[str, Any]:
        """Test handling of unavailable services"""
        start_time = time.time()
        
        try:
            # Test calling a non-existent service endpoint
            fake_socket = "/tmp/trading_nonexistent.sock"
            
            try:
                reader, writer = await asyncio.wait_for(
                    asyncio.open_unix_connection(fake_socket),
                    timeout=5.0
                )
                # Should not reach here
                return {
                    'test': 'service_unavailability',
                    'success': False,
                    'recovery_successful': False,
                    'execution_time': time.time() - start_time,
                    'error': 'Unexpected connection to non-existent service'
                }
            except (ConnectionRefusedError, FileNotFoundError, asyncio.TimeoutError):
                # Expected behavior - service properly reports unavailability
                return {
                    'test': 'service_unavailability',
                    'success': True,
                    'recovery_successful': True,
                    'execution_time': time.time() - start_time,
                    'note': 'Service unavailability properly detected'
                }
                
        except Exception as e:
            return {
                'test': 'service_unavailability',
                'success': False,
                'recovery_successful': False,
                'execution_time': time.time() - start_time,
                'error': str(e)
            }
            
    async def _test_resource_exhaustion(self) -> Dict[str, Any]:
        """Test handling of resource exhaustion scenarios"""
        start_time = time.time()
        
        try:
            # Test with large batch request to stress memory/CPU
            large_symbols = ['CBA.AX'] * 50  # Large batch
            
            result = await self._call_service('prediction', 'generate_predictions', {
                'symbols': large_symbols[:10],  # Limit to reasonable size
                'force_refresh': True
            })
            
            # Check if service handled large request gracefully
            recovery_successful = (
                result.get('success', False) or 
                'rate' in result.get('error', '').lower() or
                'limit' in result.get('error', '').lower()
            )
            
            return {
                'test': 'resource_exhaustion',
                'success': True,
                'recovery_successful': recovery_successful,
                'execution_time': time.time() - start_time,
                'note': 'Service handled large batch request appropriately'
            }
            
        except Exception as e:
            return {
                'test': 'resource_exhaustion',
                'success': False,
                'recovery_successful': False,
                'execution_time': time.time() - start_time,
                'error': str(e)
            }
            
    async def test_data_consistency(self) -> Dict[str, Any]:
        """Test data consistency across services and storage"""
        self.logger.info("ðŸ”’ Testing Data Consistency")
        
        consistency_results = {
            'category': 'data_consistency',
            'test_results': [],
            'summary': {}
        }
        
        # Test prediction data consistency
        prediction_consistency = await self._test_prediction_consistency()
        consistency_results['test_results'].append(prediction_consistency)
        
        # Test cache consistency
        cache_consistency = await self._test_cache_consistency()
        consistency_results['test_results'].append(cache_consistency)
        
        consistent_tests = sum(1 for r in consistency_results['test_results'] if r['consistent'])
        total_consistency_tests = len(consistency_results['test_results'])
        
        consistency_results['summary'] = {
            'consistent_tests': consistent_tests,
            'total_consistency_tests': total_consistency_tests,
            'consistency_rate': consistent_tests / total_consistency_tests if total_consistency_tests > 0 else 0
        }
        
        self.logger.info(f"Data consistency tests completed: {consistent_tests}/{total_consistency_tests} consistent")
        return consistency_results
        
    async def _test_prediction_consistency(self) -> Dict[str, Any]:
        """Test consistency of prediction data across multiple requests"""
        start_time = time.time()
        
        try:
            symbol = "CBA.AX"
            
            # Generate prediction twice without force refresh
            result1 = await self._call_service('prediction', 'generate_single_prediction', {
                'symbol': symbol, 
                'force_refresh': False
            })
            
            await asyncio.sleep(1)  # Small delay
            
            result2 = await self._call_service('prediction', 'generate_single_prediction', {
                'symbol': symbol,
                'force_refresh': False  
            })
            
            # Check consistency
            if result1.get('success') and result2.get('success'):
                pred1 = result1.get('result', {})
                pred2 = result2.get('result', {})
                
                # For cached predictions, results should be identical or very similar
                consistent = (
                    pred1.get('action') == pred2.get('action') and
                    abs(pred1.get('confidence', 0) - pred2.get('confidence', 0)) < 0.1
                )
            else:
                consistent = False
                
            return {
                'test': 'prediction_consistency',
                'success': result1.get('success', False) and result2.get('success', False),
                'consistent': consistent,
                'execution_time': time.time() - start_time,
                'symbol': symbol
            }
            
        except Exception as e:
            return {
                'test': 'prediction_consistency',
                'success': False,
                'consistent': False,
                'execution_time': time.time() - start_time,
                'error': str(e)
            }
            
    async def _test_cache_consistency(self) -> Dict[str, Any]:
        """Test cache consistency across services"""
        start_time = time.time()
        
        try:
            symbol = "WBC.AX"
            
            # Get cached prediction
            cached_result = await self._call_service('prediction', 'get_prediction', {'symbol': symbol})
            
            # Generate fresh prediction 
            fresh_result = await self._call_service('prediction', 'generate_single_prediction', {
                'symbol': symbol,
                'force_refresh': True
            })
            
            # Clear cache and get again
            await self._call_service('prediction', 'clear_cache', {})
            
            cleared_result = await self._call_service('prediction', 'get_prediction', {'symbol': symbol})
            
            # Analyze consistency
            cache_working = (
                cached_result.get('success', False) or 
                'no cached prediction' in cached_result.get('error', '').lower()
            )
            
            fresh_working = fresh_result.get('success', False)
            cache_cleared = not cleared_result.get('success', True)  # Should fail after clear
            
            consistent = cache_working and fresh_working and cache_cleared
            
            return {
                'test': 'cache_consistency',
                'success': True,  # Test executed
                'consistent': consistent,
                'execution_time': time.time() - start_time,
                'cache_operations': {
                    'cached_retrieval': cache_working,
                    'fresh_generation': fresh_working,
                    'cache_clearing': cache_cleared
                }
            }
            
        except Exception as e:
            return {
                'test': 'cache_consistency', 
                'success': False,
                'consistent': False,
                'execution_time': time.time() - start_time,
                'error': str(e)
            }


# Integration Test Suite Class for unittest framework
class ServiceIntegrationTestSuite(unittest.TestCase):
    """unittest-based integration test suite"""
    
    @classmethod
    def setUpClass(cls):
        """Set up test framework"""
        cls.framework = ServiceIntegrationTestFramework()
        
    def test_service_health_integration(self):
        """Test service health and availability"""
        print("ðŸ¥ Running Service Health Integration Tests...")
        
        async def run_test():
            return await self.framework.test_service_health()
            
        result = asyncio.run(run_test())
        
        # Validate results
        self.assertGreater(result['summary']['health_rate'], 0.5, 
                          "At least 50% of services should be healthy")
        
        print(f"âœ… Health test completed: {result['summary']['healthy_services']}/{result['summary']['total_services']} services healthy")
        
    def test_service_communication_integration(self):
        """Test service-to-service communication"""
        print("ðŸ“¡ Running Service Communication Integration Tests...")
        
        async def run_test():
            return await self.framework.test_service_communication()
            
        result = asyncio.run(run_test())
        
        # Validate communication
        self.assertGreater(result['summary']['communication_success_rate'], 0.6,
                          "At least 60% of service communications should succeed")
        
        print(f"âœ… Communication test completed: {result['summary']['successful_communications']}/{result['summary']['total_communications']} communications successful")
        
    def test_error_propagation_integration(self):
        """Test error handling and propagation"""
        print("âš ï¸ Running Error Propagation Integration Tests...")
        
        async def run_test():
            return await self.framework.test_error_propagation()
            
        result = asyncio.run(run_test())
        
        # Validate error handling
        self.assertGreater(result['summary']['error_handling_rate'], 0.7,
                          "At least 70% of error scenarios should be handled properly")
        
        print(f"âœ… Error propagation test completed: {result['summary']['proper_error_handling']}/{result['summary']['total_error_tests']} errors handled properly")
        
    def test_performance_integration(self):
        """Test performance under load"""
        print("ðŸš€ Running Performance Integration Tests...")
        
        async def run_test():
            return await self.framework.test_performance_under_load()
            
        result = asyncio.run(run_test())
        
        # Validate performance
        if result['test_results']:
            perf_test = result['test_results'][0]
            self.assertGreater(perf_test['success_rate'], 0.8,
                              "At least 80% of concurrent requests should succeed")
            self.assertLess(perf_test['avg_response_time'], 10.0,
                           "Average response time should be under 10 seconds")
        
        print(f"âœ… Performance test completed: {result['summary']['peak_throughput']:.2f} req/s peak throughput")
        
    def test_end_to_end_integration(self):
        """Test end-to-end workflows"""
        print("ðŸ”„ Running End-to-End Integration Tests...")
        
        async def run_test():
            return await self.framework.test_end_to_end_workflows()
            
        result = asyncio.run(run_test())
        
        # Validate workflows
        self.assertGreater(result['summary']['workflow_success_rate'], 0.5,
                          "At least 50% of end-to-end workflows should succeed")
        
        print(f"âœ… End-to-end test completed: {result['summary']['successful_workflows']}/{result['summary']['total_workflows']} workflows successful")


def run_comprehensive_integration_tests():
    """Run comprehensive integration tests and generate report"""
    print("ðŸ§ª COMPREHENSIVE SERVICE INTEGRATION TESTING FRAMEWORK")
    print("=" * 70)
    print("Testing microservice integration, communication, and reliability")
    print("=" * 70)
    
    start_time = time.time()
    
    try:
        # Initialize framework
        framework = ServiceIntegrationTestFramework()
        
        # Run all integration tests
        async def run_all_tests():
            return await framework.run_all_integration_tests()
            
        results = asyncio.run(run_all_tests())
        
        # Generate comprehensive report
        total_time = time.time() - start_time
        
        print(f"\nðŸ“Š INTEGRATION TEST RESULTS SUMMARY")
        print("=" * 50)
        print(f"ðŸ• Total Test Duration: {total_time:.2f} seconds")
        print(f"ðŸ“ˆ Overall Success Rate: {results['success_rate']:.1%}")
        print(f"âœ… Tests Passed: {results['passed_tests']}")
        print(f"âŒ Tests Failed: {results['failed_tests']}")
        print(f"ðŸ“Š Total Tests: {results['total_tests']}")
        
        print(f"\nðŸ“‹ TEST CATEGORY BREAKDOWN:")
        for category_name, category_data in results['test_categories'].items():
            category_tests = len(category_data.get('test_results', []))
            category_passed = sum(1 for test in category_data.get('test_results', []) if test.get('success', False))
            category_rate = category_passed / category_tests if category_tests > 0 else 0
            
            print(f"   {category_name.replace('_', ' ').title()}: {category_passed}/{category_tests} ({category_rate:.1%})")
        
        # Save detailed results
        results_file = f"data/integration_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        os.makedirs(os.path.dirname(results_file), exist_ok=True)
        
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\nðŸ“„ Detailed results saved to: {results_file}")
        
        # Overall assessment
        if results['success_rate'] >= 0.9:
            print("\nðŸŸ¢ EXCELLENT: Integration tests show robust microservice architecture")
        elif results['success_rate'] >= 0.7:
            print("\nðŸŸ¡ GOOD: Integration tests show stable microservice architecture with minor issues")
        elif results['success_rate'] >= 0.5:
            print("\nðŸŸ  MODERATE: Integration tests show functional architecture with improvement areas")
        else:
            print("\nðŸ”´ NEEDS IMPROVEMENT: Integration tests show significant issues requiring attention")
            
        return results['success_rate'] >= 0.7
        
    except Exception as e:
        print(f"\nâŒ INTEGRATION TEST FRAMEWORK FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # Run comprehensive integration tests
    success = run_comprehensive_integration_tests()
    
    print(f"\n{'='*70}")
    print(f"ðŸ INTEGRATION TESTING {'COMPLETED SUCCESSFULLY' if success else 'COMPLETED WITH ISSUES'}")
    print(f"{'='*70}")
    
    # Exit with appropriate code
    sys.exit(0 if success else 1)
